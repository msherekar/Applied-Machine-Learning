{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question: 1\n",
        "\n",
        "Environment, in general, is defined as anything that falls outside the agent. It communicates with the agent. It determines the reward signal for agent's action and its observation.\n",
        "\n",
        "For NIM game, specifically, the three piles, qtable and reward giving strategy based on values of reward, learning rate and discount factor constitutes an environment.\n",
        "\n",
        "Environment also includes rules via which the game is played. As an example, each player has to remove at least one item from the three piles on every turn. Then, the rule that decides the winner is also in the environment."
      ],
      "metadata": {
        "id": "4yY2HnY-rrVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO74zbJKrlrY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: 2\n",
        "\n",
        "Agent, in general, is defined as an entity that learns how to make decisions. It interacts with its surroundings by taking an action. In return for taking an action, agent receives a positive or negative feedback from its environment.\n",
        "\n",
        "In case of NIM game, there are three agents - Random, Guru and Q-Learner.\n",
        "\n",
        "Random makes random choices throughout without learning hence wins handful number of times against Qlearner and always loses to Guru. Guru makes choies based on solid mathematics and always wins against all opponents. Qlearner learns based on reinforcement learning i.e it learns using the feedback from the environment. It starts by losing games but learns evnetually to start winning."
      ],
      "metadata": {
        "id": "373gQhH3uFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T98OH2glxajM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: 3\n",
        "\n",
        "Reward and Penalty is the feedback obtained by an agent from the environment. Reward signals correct move or desired action. Penalty signals incorrect move or incorrect action.\n",
        "\n",
        "For the NIM game, value of 100 given to action upon updating of qtable is its reward. If this value was, say -100, then it will be considered as a penalty.\n"
      ],
      "metadata": {
        "id": "dFoMAkRwxbUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: 4\n",
        "\n",
        "The total number of possible states in the NIM game can be 10 x 10 x 10"
      ],
      "metadata": {
        "id": "rkVbuaae2lhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: 5\n",
        "\n",
        "The total possible unique actions for 1 player are 10 x 3 = 30"
      ],
      "metadata": {
        "id": "2TA3h4Sw5Dz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: 6\n",
        "I could not figure out an answer to this question because I didnt know how to implement my possibly incorrect correction (below). Also, I didnt quite understand these concepts. I have asked Prof.Yasin if I can submit this question later.\n",
        "\n",
        "My suggestion would be to implement Off-Policy TD control instead of equation:3 of the given code.\n",
        "\n",
        "Q(St, At) <-- Q(St, At) + alpha * [ Rt+1, gamma * max (Q(St+1, a) - Q(St,At)\n",
        "\n",
        "Guru player cannot be used inside the learnign module because it is based on mathematical rules to make sure it wins all the time. If a Qlearner is made to learn by playng against Guru, it wont learn anything because it will lose all the time.\n"
      ],
      "metadata": {
        "id": "x1LPwP3q51Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: 6"
      ],
      "metadata": {
        "id": "3xDNRhky0nUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import randint, choice\n",
        "\n",
        "# The number of piles is 3\n",
        "\n",
        "\n",
        "# max number of items per pile\n",
        "ITEMS_MX = 10\n",
        "\n",
        "# Initialize starting position\n",
        "def init_game()->list:\n",
        "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
        "\n",
        "# Based on X-oring the item counts in piles - mathematical solution\n",
        "def nim_guru(_st:list)->(int,int):\n",
        "    xored = _st[0] ^ _st[1] ^ _st[2]\n",
        "    if xored == 0:\n",
        "        return nim_random(_st)\n",
        "    for pile in range(3):\n",
        "        s = _st[pile] ^ xored\n",
        "        if s <= _st[pile]:\n",
        "            return _st[pile]-s, pile\n",
        "\n",
        "# Random Nim player\n",
        "def nim_random(_st:list)->(int,int):\n",
        "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
        "    return randint(1, _st[pile]), pile  # random move"
      ],
      "metadata": {
        "id": "TfKvsgtz0r18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nim_qlearner(_st:list)->(int,int):\n",
        "    global qtable\n",
        "    # pick the best rewarding move, equation 1\n",
        "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
        "    # index is based on move, pile\n",
        "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
        "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
        "    if move <= 0 or _st[pile] < move:\n",
        "        move, pile = nim_random(_st)  # exploration\n",
        "    return move, pile  # action\n",
        "\n",
        "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
        "\n",
        "def game(_a:str, _b:str):\n",
        "    state, side = init_game(), 'A'\n",
        "    while True:\n",
        "        engine = Engines[_a] if side == 'A' else Engines[_b]\n",
        "        move, pile = engine(state)\n",
        "        # print(state, move, pile)  # debug purposes\n",
        "        state[pile] -= move\n",
        "        if state == [0, 0, 0]:  # game ends\n",
        "            return side  # winning side\n",
        "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
        "\n",
        "def play_games(_n:int, _a:str, _b:str)->(int,int):\n",
        "    from collections import defaultdict\n",
        "    wins = defaultdict(int)\n",
        "    for _ in range(_n):\n",
        "        wins[game(_a, _b)] += 1\n",
        "    # info\n",
        "    print(f\"{_n} games, {_a:>8s}{wins['A']:5d}  {_b:>8s}{wins['B']:5d}\")\n",
        "    return wins['A'], wins['B']"
      ],
      "metadata": {
        "id": "g16JqCM-0tFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play games\n",
        "play_games(1000, 'Random', 'Random')\n",
        "play_games(1000, 'Guru', 'Random')\n",
        "play_games(1000, 'Random', 'Guru')\n",
        "play_games(1000, 'Guru', 'Guru') ;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4InyNar01GV",
        "outputId": "1c4659c2-a1ff-4980-d1dc-4645fd1b1431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 games,   Random  498    Random  502\n",
            "1000 games,     Guru 1000    Random    0\n",
            "1000 games,   Random    5      Guru  995\n",
            "1000 games,     Guru  941      Guru   59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qtable, Alpha, Gamma, Reward = None, 1, 0.8, 100.0\n",
        "\n",
        "# learn from _n games, randomly played to explore the possible states\n",
        "def nim_qlearn(_n:int):\n",
        "    global qtable\n",
        "    # based on max items per pile\n",
        "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32)\n",
        "    # play _n gameswas\n",
        "    for _ in range(_n):\n",
        "        # first state is starting position\n",
        "        st1 = init_game()\n",
        "        while True:  # while game not finished\n",
        "            # make a random move - exploration\n",
        "            move, pile = nim_random(st1)\n",
        "            st2 = list(st1)\n",
        "            # make the move\n",
        "            st2[pile] -= move  # --> last move I made\n",
        "            if st2 == [0, 0, 0]:  # game ends\n",
        "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
        "                break\n",
        "            \n",
        "            elif np.max(qtable[st2[0], st2[1], st2[2]]) >= Reward:\n",
        "                qtable_update(-Reward, st1, move, pile, np.min(qtable[st2[0], st2[1], st2[2]]))\n",
        "                # I tried Reward-10 but with only 1000 games to learn from. As per the prof, -Reward\n",
        "                # should work but it is not working.  \n",
        "                \n",
        "            else:    \n",
        "\n",
        "                qtable_update(Reward, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
        "            \n",
        "            # Switch sides for play and learning\n",
        "            st1 = st2\n",
        "\n",
        "# Equation 3 - update the qtable\n",
        "def qtable_update(r:float, _st1:list, move:int, pile:int, q_future_best:float):\n",
        "    a = pile*ITEMS_MX+move-1\n",
        "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best) # important"
      ],
      "metadata": {
        "id": "tuTWcrmY07iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nim_qlearn(1000000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrA5k-qM0-F4",
        "outputId": "825558c2-6083-4dd8-b87e-7532a4bbb308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 36s, sys: 480 ms, total: 1min 36s\n",
            "Wall time: 1min 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Play games\n",
        "play_games(1000, 'Qlearner', 'Random')\n",
        "play_games(1000, 'Random', 'Qlearner')\n",
        "play_games(1000, 'Random', 'Random') ;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sABJayzM1AzE",
        "outputId": "ce6db77f-5578-4f89-8618-ebf14fd92be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 games, Qlearner  999    Random    1\n",
            "1000 games,   Random    5  Qlearner  995\n",
            "1000 games,   Random  516    Random  484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# See the training size effect\n",
        "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
        "Wins = []\n",
        "for n in n_train:\n",
        "    nim_qlearn(n)\n",
        "    wins_a, wins_b = play_games(1000, 'Qlearner', 'Random')\n",
        "    Wins += [wins_a/(wins_a+wins_b)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t1ZsNzn1EOH",
        "outputId": "e32f790d-1b1d-4124-92f6-441f116ead44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 games, Qlearner  530    Random  470\n",
            "1000 games, Qlearner  543    Random  457\n",
            "1000 games, Qlearner  751    Random  249\n",
            "1000 games, Qlearner  928    Random   72\n",
            "1000 games, Qlearner  991    Random    9\n",
            "1000 games, Qlearner  997    Random    3\n",
            "1000 games, Qlearner  999    Random    1\n",
            "CPU times: user 14.4 s, sys: 107 ms, total: 14.6 s\n",
            "Wall time: 14.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the ratio of wins wrt to size of the reinforcement model training\n",
        "print(Wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWde_rSb1GsJ",
        "outputId": "995bf086-a3e1-4ac8-c3bf-902eefbda19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.53, 0.543, 0.751, 0.928, 0.991, 0.997, 0.999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print the entire set of states\n",
        "def qtable_log(_fn:str):\n",
        "    with open(_fn, 'w') as fout:\n",
        "        s = 'state'\n",
        "        for a in range(ITEMS_MX*3):\n",
        "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
        "            s += ',%02d_%01d' % (move,pile)\n",
        "        print(s, file=fout)\n",
        "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
        "            s = '%02d_%02d_%02d' % (i,j,k)\n",
        "            for a in range(ITEMS_MX*3):\n",
        "                r = qtable[i, j, k, a]\n",
        "                s += ',%.1f' % r\n",
        "            print(s, file=fout)\n",
        "\n",
        "qtable_log('qtable_debug.txt')"
      ],
      "metadata": {
        "id": "1G2LaVup1KAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}