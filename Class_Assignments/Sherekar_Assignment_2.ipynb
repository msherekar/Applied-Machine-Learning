{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7XpG4kmPCO5n"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBjbZ6-KCTyH"
   },
   "source": [
    "** Question: 1 **\n",
    "\n",
    "\n",
    "** Basic Definitions of all classifiers **\n",
    "\n",
    "Peceptron:\n",
    "\n",
    "It is an supervised algorithm meant for classification. It is based on the function of a neuron. A neuron takes in multliple biochemical signals and fires a signal once signals inside it reach a threshold. Similary, a perceptron takes in all the features of the data, multiplies by their respective weights and comes to a decision. This decision (yes/no, 0/1 or -1/+1) is based on a function called a Decision Function. weights are adjusted in subsequent iterations. The objective of a perceptron is to minimize misclassification errors. It does this by separating the datapoints by creating a hyperplane.\n",
    "\n",
    "Support Vector Machine(SVM):\n",
    "\n",
    "A support vector is the data point closest to the hyperplanes. Goal of a SVM algorithm is to maximize distance between these support vectors and decision boundary. The advantage of such maximization is lower generalization leading to optimal fitting. In other words, SVM is a linear programming problem where the goal is to maximize distance between two support vectors on either side of a decision boundary.\n",
    "\n",
    "Decision Tree:\n",
    "\n",
    "A decision tree classifier, as the name suggests, works on the principle of decision. This decision could be a binary choice like yes/no or ternary choice etc. A decision is being made at each level and finally aleaf node is reached which determines the classification. So, every row in the dataset is subjected to decision for each feature and arrives at leaf node which is nothing but its label.\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "It is a decision tree based algorithm and as the name suggests is a combination (ensemble) of randomly selected multiple decision trees. Each decision tree starts at a random node(row/data point). Decision for each node is made by \"majority\" vote i.e when the mmodel is applied on a new data point, several decisions are made on classifying it by different trees (multiple trees i.e multiple rows) are chosen and whatever the \"majority\" says, becomes the label of the new data point.\n",
    "\n",
    "** Comparison & Contrast **\n",
    "\n",
    "Perceptron Vs SVM\n",
    "\n",
    "- Perceptron and SVM solve optimization problem. The problem is placing a hyperplane that separates two clusters of data points. The difference between the two is this placing of the hyperplane. Perceptron just cares about separating i.e placing but it does not care about the location of placing. Whereas SVM tries to place this hyperplane at the best location possible. \n",
    "\n",
    "SVM is like a sharp knife able to cut throught small complex datasets (how small?). Super accurate with less computation power. It can do so because of its ability to project/transform data into higher dimension. If SVM is a sharp knife, Perceptron is a hammer !\n",
    "\n",
    "Perceptron can be trained \"on the go\" while SVM cannot be. SVM cannot be perform multiclass classificatation directly. Also, SVM cannot predict the probability of a class directly. It can be done indirectly using Platt scaling.\n",
    "\n",
    "Decision Tree Vs Random Forest\n",
    "\n",
    "Decision Tree: Main advantage is its intrepretability i.e the process of explaining the model is very easy because one can easily see which feature were used to split. But a tree cannot be too deep becuse that will create computational burden and confusion and will take away the intrepretability advantage. Hence, the question becomes what should be the ideal depth so that overfitting doesnt happen.\n",
    "\n",
    "Random Forest: This classifer has good classification performance and  it is robust toward overfitting, good scalability and easy to use. Main reason for not overfitting is using multiple trees to arrive at a decison but this takes away interpretability. It does not require to choose hyperparameters. \n",
    "\n",
    "A decision tree tends to overfit but Random Forrest corrects this by using random feature selection. Random Forest\n",
    "maintains accurancy even in absense of some data. It can be used for either classification or regression. It requires lots of memory for larger datasets so it can be slower at times. RF can assign importance to each feature. Overall, it is easy to train, specially in early stages, but slow to apply to a new dataset. It can only predict but not describe relationship between data whereas decision tree can do the later.\n",
    "\n",
    "\n",
    "Perceptron/SVM Vs Decision Tree/Random Forest\n",
    "\n",
    "Perceptron/SVM use cost function while Decision Tree/Random Forest use gini impurity, entropy and classificatio error.\n",
    "Tree based sructure captures interactions between features; has natural visualization.\n",
    "Tree based fails to deal with linear relationships thatswhere SVM can come in handy. Tree based are unstable i.e very dependent on training data set. Splits are dependent on which feature is chosen first. Whereas SVM/Perceptron classifiers dont have these pitfalls and hence are very robust and stable against nature of data.\n",
    "\n",
    "\n",
    "\n",
    "Which one will be the first that you would try on your dataset?\n",
    "\n",
    "The answer to this question depends on the nature of the dataset. Hence, that will have to be determined first. Questions to be answered will be about numerical/nominal nature, then linear/non-linearity and finally amount/volume of data.\n",
    "\n",
    "If I have to choose one out the four given, I will start with SVM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_vzbLiq2y5T"
   },
   "source": [
    "Question: 2 ** Features **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oqyRZnaGa5z"
   },
   "source": [
    "Numerical: Feature types that have integers or floating types as data type. They can bre represented as int or float. GRE score data is an exmaple of numerical data.\n",
    "\n",
    "Nominal: Features types that are drawn from a finite set of levels or from an alphabet. They can converted to int or float using hot encoding. Idea is to use a code to have these nominal values as numbers. Rcuisine data is type of nominal data.\n",
    "\n",
    "Date: Date data format can come in varitey of ways like mm/dd/yy or mm/dd/yyyy or dd/mm/yy etc. This represents three features than can be easily extracted and used as numerical features. Date column is exmaple of Date data.\n",
    "\n",
    "Text: A column that contains alphabets like aminoacid names. Or columns have words that implies certain meaning. For example, Large with respect shir sizes. The general idea would be to convert words into numbers by using certain code. For example protein name Valine can be given a code 0, Proline can be give a code 1 so on and so forth. Rcuisine data can again be taken as exaple of text data type.`\n",
    "\n",
    "Image: An image is represented by pixels. Pixels can represnt the three colors (red, blue, green) or just (black and white).\n",
    "\n",
    "Dependent variable: It is generally the label or predicted value. It is represented as a separate matrix of rows equal to rows in feature matrix and one single column. Chance of Admit column is example of dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/Users/mukulsherekar/Documents/Applied_Machine_Learning/admission_data.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>Rcuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135110</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135109</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135107</td>\n",
       "      <td>Latin_American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135106</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135105</td>\n",
       "      <td>Fast_Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   placeID        Rcuisine\n",
       "0   135110         Spanish\n",
       "1   135109         Italian\n",
       "2   135107  Latin_American\n",
       "3   135106         Mexican\n",
       "4   135105       Fast_Food"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('/Users/mukulsherekar/Downloads/restaurant/chefmozcuisine.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vcfI5CEoQznU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>tournament</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1872-11-30</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>England</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Friendly</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1873-03-08</td>\n",
       "      <td>England</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Friendly</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1874-03-07</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>England</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Friendly</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1875-03-06</td>\n",
       "      <td>England</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Friendly</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1876-03-04</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>England</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Friendly</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date home_team away_team  home_score  away_score tournament     city  \\\n",
       "0  1872-11-30  Scotland   England           0           0   Friendly  Glasgow   \n",
       "1  1873-03-08   England  Scotland           4           2   Friendly   London   \n",
       "2  1874-03-07  Scotland   England           2           1   Friendly  Glasgow   \n",
       "3  1875-03-06   England  Scotland           2           2   Friendly   London   \n",
       "4  1876-03-04  Scotland   England           3           0   Friendly  Glasgow   \n",
       "\n",
       "    country  neutral  \n",
       "0  Scotland    False  \n",
       "1   England    False  \n",
       "2  Scotland    False  \n",
       "3   England    False  \n",
       "4  Scotland    False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples using code\n",
    "\n",
    "df3 = pd.read_csv('/Users/mukulsherekar/Downloads/football/results.csv')\n",
    "df3.head()\n",
    "\n",
    "#img1 = cv2.imread('/Users/mukulsherekar/Downloads/pokemon/images/images/abomasnow.png')\n",
    "#img2 = cv2.imread('/Users/mukulsherekar/Downloads/pokemon/images/images/abra.png')\n",
    "\n",
    "#df4 = pd.DataFrame()\n",
    "#df4['img'] = [img1] # Wrap image in python list\n",
    "\n",
    "#df4.head()\n",
    "\n",
    "# code is not running for images for some reason that I Cannot troubleshoot\n",
    "# but this is how one can add images to a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE75Lvi3QCb_"
   },
   "source": [
    "** Question: 3 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yZ14kT_SRpq"
   },
   "source": [
    "Confusion Matrix: This matrix has four values namely True Positives (TP), True Negatives (TN), False Positives (FP), True NEgatives (TN). \n",
    "\n",
    "TP: Those predictions that were positives and were labelled as such\n",
    "\n",
    "TN: Those predictions that were negatives and were labelled as such\n",
    "\n",
    "FP: Those predictions that were not positives and were labelled as such\n",
    "\n",
    "FN: Those predictions that were not negatives and were labelled as such\n",
    "\n",
    "Precision:\n",
    "It is a ratio that quantifies how may records predicted as positives were truly positive\n",
    "     PRE = TP/TP + FP\n",
    "\n",
    "Recall: True Positive Rate\n",
    "It is a ratio that quantifies how many records that were predicted correctly as positives out of all positives (false negatives + true positives)\n",
    "     REC = TP/TP + FN\n",
    "\n",
    "F1 score: Harmonic mean of precision and recall is calle F1 score.\n",
    "      F1 = 2 x PRE x REC / (PRE + REC)\n",
    "\n",
    "Matthews Correlation Coefficient (ranges from -1 to 1)\n",
    "  MCC = TP X TN - FP x FN / sqrt((TP+FP) (TP+FN) (TN + FP) (TN+FN)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE_UIa_bWcXF"
   },
   "source": [
    "** Question: 4 **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate and load the data file\n",
    "df = pd.read_csv('/Users/mukulsherekar/Documents/Applied_Machine_Learning/admission_data.csv')\n",
    "column_names = list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# STANDARD DEVIATION\n",
    "\n",
    "def std_dev(N):\n",
    "    \n",
    "    column_names = list(N)\n",
    "    \n",
    "    SD = []\n",
    "    \n",
    "    for i in column_names:\n",
    "        c = df[i]\n",
    "        C = np.array(c)\n",
    "        s = []\n",
    "        for i in C:\n",
    "            diff = i-C.mean()\n",
    "            s.append(diff*diff)\n",
    "\n",
    "        s_elements = sum(s)\n",
    "        sq_sd = s_elements/len(C)\n",
    "        sd = math.sqrt(sq_sd)\n",
    "        SD.append(sd)\n",
    "    return SD\n",
    "\n",
    "# CO-VARIANCE B/W TWO FEATURES\n",
    "\n",
    "def covariance(f1,f2): # f1 and f2 are two columns (features) of the form df['col_name']\n",
    "    F1 = np.array(f1)\n",
    "    F2 = np.array(f2)\n",
    "\n",
    "    m_F1 = F1.mean()\n",
    "    m_F2 = F2.mean()\n",
    "\n",
    "\n",
    "    sum_F1 = [i - m_F1 for i in F1]\n",
    "    sum_F2 = [i - m_F2 for i in F2]\n",
    "\n",
    "    sum_value = sum([sum_F1[i]*sum_F2[i] for i in range(len(F1))])\n",
    "\n",
    "    denominator = len(F1) - 1\n",
    "    \n",
    "    cov = sum_value / denominator\n",
    "\n",
    "    return cov\n",
    "\n",
    "# COVARIANCE VALUES FOR THE WHOLE DATAFRAME\n",
    "\n",
    "def cov_df(N): # N is name of the dataframe\n",
    "    covariance_N = [[covariance(df[a],df[b]) for a in column_names] for b in column_names]\n",
    "    cov = np.array(covariance_N)\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'column_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5c3b6d2154dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# printing covariance for the given data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2bee9a8e85b6>\u001b[0m in \u001b[0;36mcov_df\u001b[0;34m(N)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcov_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# N is name of the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mcovariance_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'column_names' is not defined"
     ]
    }
   ],
   "source": [
    "# printing covariance for the given data \n",
    "print(cov_df(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_dev(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I couldnt get corelation matrix from this point on so using df.corr to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the diagonal values are all 1 because each feature is being compared with itself. \n",
    "# the last column is the most important in deciding which features have the most prediction\n",
    "# power. this is because each feature is being compared with chance of admit to obtain\n",
    "# co-relation coefficient. CGPA has the highest co-relation with chance of admit so\n",
    "# it has the highest predictive power among all features."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
