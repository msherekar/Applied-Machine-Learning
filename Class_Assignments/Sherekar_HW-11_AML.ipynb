{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0187a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import ranf\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', 'Solver terminated early.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b903a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    from numpy import fromfile, uint8\n",
    "    import os\n",
    "    import struct\n",
    "\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = fromfile(lbpath, dtype=uint8)\n",
    "        with open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "            images = fromfile(imgpath, dtype=uint8).reshape(len(labels), 784)\n",
    "            images = ((images / 255.) - .5) * 2\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da35bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 784\n",
      "Rows= 10000, columns= 784\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train_mnist, y_train_mnist = load_mnist('/Users/mukulsherekar/Downloads/mnist/', kind='train')\n",
    "print(f'Rows= {X_train_mnist.shape[0]}, columns= {X_train_mnist.shape[1]}')\n",
    "\n",
    "X_test_mnist, y_test_mnist = load_mnist('/Users/mukulsherekar/Downloads/mnist/', kind='t10k')\n",
    "print(f'Rows= {X_test_mnist.shape[0]}, columns= {X_test_mnist.shape[1]}')\n",
    "\n",
    "np.savez_compressed('mnist_scaled.npz',\n",
    "                    X_train=X_train_mnist,\n",
    "                    y_train=y_train_mnist,\n",
    "                    X_test=X_test_mnist,\n",
    "                    y_test=y_test_mnist)\n",
    "mnist = np.load('mnist_scaled.npz')\n",
    "X_train, y_train, X_test, y_test = [mnist[f] for f in ['X_train', 'y_train',\n",
    "                                                       'X_test', 'y_test']]\n",
    "\n",
    "print(X_train_mnist.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4c7b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300/300 | Cost: 13503.52 | Train/Valid Acc.: 96.66%/94.98% "
     ]
    }
   ],
   "source": [
    "### Implementing a multi-layer perceptron\n",
    "class NeuralNetMLP(object):\n",
    "\n",
    "    def __init__(self, n_hidden=30, epochs=100, eta=0.001, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)  # used to randomize weights\n",
    "        self.n_hidden = n_hidden  # size of the hidden layer\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch - 1 would not work\n",
    "        self.w_out, self.w_h1, self.w_h2 = None, None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def onehot(_y, _n_classes):  # one hot encode the input class y\n",
    "        onehot = np.zeros((_n_classes, _y.shape[0]))\n",
    "        for idx, val in enumerate(_y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(_z):  # Eq 1\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(_z, -250, 250)))\n",
    "\n",
    "    def _forward(self, _X):  # Eq 2\n",
    "        z_h1 = np.dot(_X, self.w_h1)  # input into hidden layer-1\n",
    "        a_h1 = self.sigmoid(z_h1)  # activation of hidden layer-1\n",
    "\n",
    "        z_h2 = np.dot(a_h1, self.w_h2)  # input into hidden layer-2\n",
    "        a_h2 = self.sigmoid(z_h2)  # activation of hidden layer-2\n",
    "\n",
    "        z_out = np.dot(a_h2, self.w_out) # input into output layer\n",
    "        a_out = self.sigmoid(z_out)      # activation of output layer\n",
    "        return z_h1, a_h1, z_h2, a_h2, z_out, a_out\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):  # Eq 4\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, _X):\n",
    "        z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(_X)\n",
    "        ypred = np.argmax(z_out, axis=1)\n",
    "        return ypred\n",
    "\n",
    "    def fit(self, _X_train, _y_train, _X_valid, _y_valid):\n",
    "        import sys\n",
    "        n_output = np.unique(_y_train).shape[0]  # number of class labels\n",
    "        n_features = _X_train.shape[1]\n",
    "        \n",
    "        # weights of two hidden layers\n",
    "        self.w_h1 = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden))\n",
    "        self.w_h2 = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        \n",
    "        # weights of output layer\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(n_output, self.n_hidden))\n",
    "\n",
    "        y_train_enc = self.onehot(_y_train, self.n_hidden)  # one-hot encode original y\n",
    "        \n",
    "        for ei in range(self.epochs):  # Ideally must shuffle at every epoch\n",
    "            indices = np.arange(_X_train.shape[0])\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "\n",
    "                # Backpropagation\n",
    "                z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(_X_train[batch_idx])  # neural network model\n",
    "\n",
    "                # output layer into 2nd hidden layer\n",
    "                delta_out = a_out - y_train_enc[batch_idx]  \n",
    "\n",
    "                grad_w_out = np.dot(a_h2.T, delta_out)  \n",
    "                self.w_out -= self.eta * grad_w_out\n",
    "\n",
    "                # 2nd hidden layer into 1st hidden layer\n",
    "                sigmoid_derivative_h2 = a_h2 * (1.0 - a_h2)  \n",
    "                delta_h2 = (np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h2)  \n",
    "                grad_w_h2 = np.dot(a_h1.T, delta_h2)  \n",
    "                self.w_h2 -= self.eta * grad_w_h2  \n",
    "\n",
    "                # 1st hidden layer into input layer\n",
    "                sigmoid_derivative_h1 = a_h1 * (1.0 - a_h1)  \n",
    "                delta_h1 = (np.dot(delta_h2, self.w_h2.T) * sigmoid_derivative_h1)  \n",
    "                grad_w_h1 = np.dot(_X_train[batch_idx].T, delta_h1)  \n",
    "                self.w_h1 -= self.eta * grad_w_h1\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(_X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(_X_train)  # monitoring training progress through reclassification\n",
    "            y_valid_pred = self.predict(_X_valid)  # monitoring training progress through validation\n",
    "            train_acc = ((np.sum(_y_train == y_train_pred)).astype(float) / _X_train.shape[0])\n",
    "            valid_acc = ((np.sum(_y_valid == y_valid_pred)).astype(float) / _X_valid.shape[0])\n",
    "            sys.stderr.write('\\r%d/%d | Cost: %.2f ' '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
    "                             (ei + 1, self.epochs, cost, train_acc * 100, valid_acc * 100))\n",
    "            sys.stderr.flush()\n",
    "        return self\n",
    "\n",
    "\n",
    "# Define and fit the neural network\n",
    "nn = NeuralNetMLP(n_hidden=20, epochs=300, eta=0.0005, minibatch_size=100, seed=123)\n",
    "nn.fit(X_train[:55000], y_train[:55000], X_train[55000:], y_train[55000:])\n",
    "\n",
    "\n",
    "def get_acc(_y_test, _y_pred):\n",
    "    return (np.sum(_y_test == _y_pred)).astype(float) / _y_test.shape[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
